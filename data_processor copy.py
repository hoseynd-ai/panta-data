import pandas as pd
import re
from typing import List, Dict, Any, Tuple
from collections import defaultdict
import streamlit as st

class DataProcessor:
    def __init__(self):
        self.df = None
        self.processed_data = None
        self.excel_file_path = "excel_files_by_year_panta-new.xlsx"
        
        # Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø­Ø°Ù Ø´ÙˆÙ†Ø¯
        self.keywords_to_remove = [
            "Ù…Ø¯ÛŒØ± ÙØ±ÙˆØ´",
            "Ù‡Ø²ÛŒÙ†Ù‡ Ø­Ù…Ù„",
            "ØªÙˆØ¶ÛŒØ­Ø§Øª",
            "Ù…Ø¯ÛŒØ± Ú©Ø§Ø±Ø®Ø§Ù†Ù‡",
            "Ú©Ø¯ Ú©Ø§Ù„Ø§",
            "Ø´Ø±Ø­ Ú©Ø§Ù„Ø§",
            "Ù…ÙˆØ¬ÙˆØ¯ÛŒ",
            "ØªØ§Ø±ÛŒØ® Ø§Ø±Ø³Ø§Ù„",
            "Ù…Ø¯ÛŒØ± Ø¹Ø§Ù…Ù„",
            "Ø§Ø­ØªØ±Ø§Ù…Ø§ØŒ Ø¯Ø³ØªÙˆØ± ÙØ±Ù…Ø§ÛŒÛŒØ¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¬Ø§Ø±ÛŒ Ø¬Ù‡Øª Ø®Ø±ÛŒØ¯Ø§Ø± Ø°ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ Ú¯Ø±Ø¯Ø¯",
            "ØªÙˆØ²ÛŒØ¹ Ù†Ø³Ø®",
            "ÙˆØ§Ø­Ø¯ ÙØ±ÙˆØ´",
            "Ø±Ø¯ÛŒÙ",
            "|",
            "ØªÙˆØ²ÛŒØ¹ Ù†Ø³Ø®",
            "Ú¯Ø²Ø§Ø±Ø´ Ø²Ù…Ø§Ù† Ø¨Ù†Ø¯ÛŒ Ø§Ø±Ø³Ø§Ù„"
        ]
    
    def unify_persian_chars(self, s: str) -> str:
        """ÛŒÚ©Ø³Ø§Ù†â€ŒØ³Ø§Ø²ÛŒ Ø­Ø±ÙˆÙ Ø¹Ø±Ø¨ÛŒ/ÙØ§Ø±Ø³ÛŒ"""
        if s is None:
            return ""
        s = str(s)
        s = s.replace("\u064A", "ÛŒ").replace("\u0643", "Ú©")
        return s
    
    def remove_keywords(self, text: str) -> str:
        """Ø­Ø°Ù Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ Ø§Ø² Ù…ØªÙ†"""
        if pd.isna(text) or not isinstance(text, str):
            return ""
        
        cleaned_text = text
        for keyword in self.keywords_to_remove:
            # Ø­Ø°Ù Ø¯Ù‚ÛŒÙ‚ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡
            cleaned_text = cleaned_text.replace(keyword, "")
        
        # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ ÙØ¶Ø§Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
        return cleaned_text
    
    def normalize_status(self, s) -> str:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙˆØ¶Ø¹ÛŒØª Ø±Ø³Ù…ÛŒ/ØºÛŒØ±Ø±Ø³Ù…ÛŒ Ø¨Ø§ Ø¯Ù‚Øª Ø¨ÛŒØ´ØªØ± Ø¯Ø± ØªØ´Ø®ÛŒØµ"""
        if pd.isna(s):
            return "Ù†Ø§Ù…Ø´Ø®Øµ"
        
        st_text = self.unify_persian_chars(str(s)).strip()
        
        # Ø­Ø°Ù ØªÙ…Ø§Ù… ÙØ¶Ø§Ù‡Ø§ØŒ Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ Ùˆ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
        st_nospace = re.sub(r"[\s\-\u200c\u200d_:ØŒ,\u00A0\u2000-\u200A\u202F\u205F\u3000]+", "", st_text)
        st_nospace = st_nospace.lower()
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø¨Ø±Ø§ÛŒ "ØºÛŒØ± Ø±Ø³Ù…ÛŒ" Ùˆ "ØºÛŒØ±Ø±Ø³Ù…ÛŒ"
        if re.search(r"ØºÛŒØ±.*Ø±Ø³Ù…", st_nospace) or re.search(r"ØºÙŠØ±.*Ø±Ø³Ù…", st_nospace):
            return "ØºÛŒØ±Ø±Ø³Ù…ÛŒ"
        elif re.search(r"Ø±Ø³Ù…", st_nospace) and not re.search(r"ØºÛŒØ±|ØºÙŠØ±", st_nospace):
            return "Ø±Ø³Ù…ÛŒ"
        elif st_nospace == "" or "Ù†Ø§Ù…Ø´Ø®Øµ" in st_nospace:
            return "Ù†Ø§Ù…Ø´Ø®Øµ"
        
        return "Ù†Ø§Ù…Ø´Ø®Øµ"
    
    def clean_text(self, text: str) -> str:
        """Ø­Ø°Ù Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ø­Ø±ÙˆÙ ÙØ§Ø±Ø³ÛŒ"""
        if pd.isna(text):
            return ""
        txt = self.unify_persian_chars(str(text))
        return re.sub(r"[^Ø¢-ÛŒ\s]", "", txt)
    
    def load_data(self, file_path: str = None) -> pd.DataFrame:
        """Load Excel data from file"""
        try:
            if file_path is None:
                file_path = self.excel_file_path
            
            # Read all sheets from Excel file
            excel_file = pd.ExcelFile(file_path)
            all_sheets_data = []
            
            st.info(f"ğŸ“Š Ø¯Ø± Ø­Ø§Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† {len(excel_file.sheet_names)} Ø´ÛŒØª Ø§Ø² ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„...")
            
            for sheet_name in excel_file.sheet_names:
                try:
                    sheet_df = pd.read_excel(file_path, sheet_name=sheet_name)
                    sheet_df['sheet_name'] = sheet_name
                    
                    # Try to identify filename column
                    filename_col = None
                    for col in sheet_df.columns:
                        if any(keyword in str(col).lower() for keyword in ['filename', 'file', 'Ù†Ø§Ù…', 'ÙØ§ÛŒÙ„']):
                            filename_col = col
                            break
                    
                    if filename_col is None and len(sheet_df.columns) > 0:
                        filename_col = sheet_df.columns[0]
                    
                    if filename_col:
                        sheet_df['filename'] = sheet_df[filename_col]
                    else:
                        sheet_df['filename'] = f"Row_{sheet_df.index}"
                    
                    # Create file_content column by combining all text columns
                    text_columns = [col for col in sheet_df.columns if sheet_df[col].dtype == 'object']
                    sheet_df['file_content'] = sheet_df[text_columns].apply(
                        lambda row: ' '.join([str(val) for val in row if pd.notna(val)]), axis=1
                    )
                    
                    # Ø§Ø¹Ù…Ø§Ù„ Ø­Ø°Ù Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ù…Ø­ØªÙˆØ§ÛŒ ÙØ§ÛŒÙ„
                    sheet_df['file_content'] = sheet_df['file_content'].apply(self.remove_keywords)
                    
                    all_sheets_data.append(sheet_df)
                    
                except Exception as e:
                    st.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† Ø´ÛŒØª {sheet_name}: {str(e)}")
            
            if all_sheets_data:
                self.df = pd.concat(all_sheets_data, ignore_index=True)
                st.success(f"âœ… ÙØ§ÛŒÙ„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯: {len(self.df)} Ø±Ú©ÙˆØ±Ø¯ Ø§Ø² {len(excel_file.sheet_names)} Ø´ÛŒØª")
            else:
                st.error("Ù‡ÛŒÚ† Ø´ÛŒØª Ù‚Ø§Ø¨Ù„ Ø®ÙˆØ§Ù†Ø¯Ù†ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
                return None
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
            if "official_status" not in self.df.columns:
                self.df["official_status"] = None
            if "month" not in self.df.columns:
                self.df["month"] = None
            if "year" not in self.df.columns:
                self.df["year"] = None
            
            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            self.df["official_status"] = self.df["official_status"].apply(self.normalize_status)
            self.df["clean_name"] = self.df["filename"].apply(self.clean_text)
            self.df["month"] = self.df["month"].fillna("Ù†Ø§Ù…Ø´Ø®Øµ")
            self.df["year_num"] = pd.to_numeric(self.df["year"], errors="coerce").astype("Int64")
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ù…Ø§Ø±Ù‡ ØªÙ„ÙÙ†
            self.df["extracted_phones"] = self.df["file_content"].apply(self.extract_phone_numbers)
            self.df["phone_count"] = self.df["extracted_phones"].apply(len)
            self.df["phone_numbers_str"] = self.df["extracted_phones"].apply(lambda x: ', '.join(x) if x else 'ÛŒØ§ÙØª Ù†Ø´Ø¯')
            
            # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ø´ÛŒØªâ€ŒÙ‡Ø§
            sheet_stats = self.df['sheet_name'].value_counts().sort_index()
            st.info(f"ğŸ“Š ØªÙÚ©ÛŒÚ© Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´ÛŒØª:\n" + 
                   "\n".join([f"   - {sheet}: {count} Ø±Ú©ÙˆØ±Ø¯" for sheet, count in sheet_stats.items()]))
            
            return self.df
        except Exception as e:
            st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {str(e)}")
            return None
    
    def extract_phone_numbers(self, text: str) -> List[str]:
        """Extract phone numbers from text using regex patterns"""
        if pd.isna(text) or not isinstance(text, str):
            return []
        
        # Persian/Iranian phone number patterns
        patterns = [
            r'09\d{9}',  # Mobile numbers starting with 09
            r'\+989\d{9}',  # International format
            r'0\d{2,3}-?\d{7,8}',  # Landline numbers
            r'\b\d{11}\b',  # 11-digit numbers
            r'\b\d{4}-?\d{7}\b',  # Numbers with dash
        ]
        
        phone_numbers = []
        for pattern in patterns:
            matches = re.findall(pattern, text)
            phone_numbers.extend(matches)
        
        # Remove duplicates and return
        return list(set(phone_numbers))
    
    def extract_company_names(self, filename: str, content: str) -> List[str]:
        """Extract company names from filename and content"""
        companies = []
        
        # Extract from filename
        if filename:
            name_parts = str(filename).replace('.xlsm', '').replace('.xlsx', '')
            parts = [part.strip() for part in name_parts.split('-') if len(part.strip()) > 2]
            companies.extend(parts)
        
        # Extract from content
        if isinstance(content, str):
            buyer_pattern = r'Ø®Ø±ÛŒØ¯Ø§Ø±\s*:?\s*([^,;\n]+)'
            buyer_matches = re.findall(buyer_pattern, content)
            companies.extend([match.strip() for match in buyer_matches if match.strip()])
        
        return list(set(companies))
    
    def extract_products(self, text: str) -> List[str]:
        """Extract product information from text"""
        if pd.isna(text) or not isinstance(text, str):
            return []
        
        # Product keywords in Persian
        product_keywords = [
            'Ù…Ø­ØµÙˆÙ„', 'Ú©Ø§Ù„Ø§', 'Ø¬Ù†Ø³', 'Ø§Ù‚Ù„Ø§Ù…', 'Ù…ÙˆØ§Ø¯', 'ØªØ¬Ù‡ÛŒØ²Ø§Øª',
            'Ø¯Ø³ØªÚ¯Ø§Ù‡', 'Ù…Ø§Ø´ÛŒÙ†', 'Ù‚Ø·Ø¹Ù‡', 'Ù„ÙˆØ§Ø²Ù…', 'Ø§Ø¨Ø²Ø§Ø±', 'Ø³ÛŒØ³ØªÙ…',
            'Ù†Ø±Ù… Ø§ÙØ²Ø§Ø±', 'Ø¨Ø±Ù†Ø§Ù…Ù‡', 'Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù†', 'Ù¾Ù„ØªÙØ±Ù…', 'Ø®Ø¯Ù…Ø§Øª'
        ]
        
        products = []
        words = text.split()
        
        for i, word in enumerate(words):
            if any(keyword in word for keyword in product_keywords):
                context_start = max(0, i-2)
                context_end = min(len(words), i+3)
                context = ' '.join(words[context_start:context_end])
                products.append(context.strip())
        
        return list(set(products))
    
    def fuzzy_search_companies(self, query: str, threshold: int = 60) -> List[Tuple[str, int]]:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ ÙØ§Ø²ÛŒ Ø´Ø±Ú©Øªâ€ŒÙ‡Ø§"""
        if not query or self.df is None:
            return []
        
        query_clean = self.clean_text(query).lower()
        companies = self.df["clean_name"].dropna().unique().tolist()
        
        matches = []
        for company in companies:
            company_clean = company.lower()
            
            if query_clean in company_clean:
                score = 100
            elif any(word in company_clean for word in query_clean.split()):
                score = 80
            else:
                common_chars = set(query_clean) & set(company_clean)
                score = int((len(common_chars) / max(len(query_clean), len(company_clean))) * 100)
            
            if score >= threshold:
                matches.append((company, score))
        
        matches.sort(key=lambda x: x[1], reverse=True)
        return matches[:15]
    
    def process_data(self) -> pd.DataFrame:
        """Process the raw data and extract structured information"""
        if self.df is None:
            return None
        
        processed_rows = []
        
        for _, row in self.df.iterrows():
            # Ø§Ø¹Ù…Ø§Ù„ Ø­Ø°Ù Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ù…Ø­ØªÙˆØ§ÛŒ ÙØ§ÛŒÙ„ Ù‚Ø¨Ù„ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´
            cleaned_content = self.remove_keywords(str(row.get('file_content', '')))
            
            phones = self.extract_phone_numbers(cleaned_content)
            companies = self.extract_company_names(
                str(row.get('filename', '')), 
                cleaned_content
            )
            products = self.extract_products(cleaned_content)
            
            processed_row = {
                'Ø´Ù…Ø§Ø±Ù‡_Ø³Ù†Ø¯': str(row.get('filename', '')).split('-')[0] if '-' in str(row.get('filename', '')) else '',
                'Ù†Ø§Ù…_Ø´Ø±Ú©Øª': ', '.join(companies) if companies else 'Ù†Ø§Ù…Ø´Ø®Øµ',
                'Ù†Ø§Ù…_Ù¾Ø§Ú©': row.get('clean_name', ''),
                'Ø³Ø§Ù„': row.get('year', ''),
                'Ø³Ø§Ù„_Ø¹Ø¯Ø¯ÛŒ': row.get('year_num', ''),
                'Ù…Ø§Ù‡': row.get('month', ''),
                'ÙˆØ¶Ø¹ÛŒØª': row.get('official_status', 'Ù†Ø§Ù…Ø´Ø®Øµ'),
                'Ø´ÛŒØª_Ù…Ù†Ø¨Ø¹': row.get('sheet_name', ''),
                'Ø´Ù…Ø§Ø±Ù‡_ØªÙ„ÙÙ†': ', '.join(phones) if phones else 'ÛŒØ§ÙØª Ù†Ø´Ø¯',
                'ØªØ¹Ø¯Ø§Ø¯_ØªÙ„ÙÙ†': len(phones),
                'Ù…Ø­ØµÙˆÙ„Ø§Øª': ', '.join(products) if products else 'ÛŒØ§ÙØª Ù†Ø´Ø¯',
                'ØªØ¹Ø¯Ø§Ø¯_Ù…Ø­ØµÙˆÙ„Ø§Øª': len(products),
                'ÙØ§ÛŒÙ„_Ø§ØµÙ„ÛŒ': row.get('filename', ''),
                'Ù…Ø­ØªÙˆØ§ÛŒ_Ú©Ø§Ù…Ù„': cleaned_content  # Ù…Ø­ØªÙˆØ§ÛŒ Ù¾Ø§Ú© Ø´Ø¯Ù‡ Ø±Ø§ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            }
            
            processed_rows.append(processed_row)
        
        self.processed_data = pd.DataFrame(processed_rows)
        return self.processed_data
    
    def search_company_detailed(self, company_name: str) -> Dict[str, Any]:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ ØªÙØµÛŒÙ„ÛŒ ÛŒÚ© Ø´Ø±Ú©Øª"""
        if self.processed_data is None:
            return {}
        
        matches = self.fuzzy_search_companies(company_name)
        if not matches:
            return {"error": f"Ù‡ÛŒÚ† Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ '{company_name}' ÛŒØ§ÙØª Ù†Ø´Ø¯"}
        
        best_match = matches[0][0]
        best_score = matches[0][1]
        
        results = self.processed_data[
            self.processed_data["Ù†Ø§Ù…_Ù¾Ø§Ú©"].str.contains(re.escape(best_match), na=False)
        ]
        
        if results.empty:
            return {"error": f"Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ÛŒ Ø¨Ø±Ø§ÛŒ '{best_match}' ÛŒØ§ÙØª Ù†Ø´Ø¯"}
        
        total_files = len(results)
        years_active = sorted(results["Ø³Ø§Ù„_Ø¹Ø¯Ø¯ÛŒ"].dropna().unique().astype(int).tolist())
        status_counts = results["ÙˆØ¶Ø¹ÛŒØª"].value_counts()
        sheet_distribution = results["Ø´ÛŒØª_Ù…Ù†Ø¨Ø¹"].value_counts()
        
        yearly_stats = defaultdict(lambda: {"total": 0, "Ø±Ø³Ù…ÛŒ": 0, "ØºÛŒØ±Ø±Ø³Ù…ÛŒ": 0, "Ù†Ø§Ù…Ø´Ø®Øµ": 0, "months": set(), "phones": 0})
        for _, row in results.iterrows():
            yr = row["Ø³Ø§Ù„_Ø¹Ø¯Ø¯ÛŒ"]
            if pd.isna(yr):
                continue
            yr = int(yr)
            st_status = row["ÙˆØ¶Ø¹ÛŒØª"]
            monthly = row["Ù…Ø§Ù‡"]
            yearly_stats[yr]["total"] += 1
            yearly_stats[yr][st_status] += 1
            yearly_stats[yr]["months"].add(str(monthly))
            yearly_stats[yr]["phones"] += row["ØªØ¹Ø¯Ø§Ø¯_ØªÙ„ÙÙ†"]
        
        trend_analysis = "Ø«Ø§Ø¨Øª"
        if len(years_active) > 1:
            first = yearly_stats[years_active[0]]["total"]
            last = yearly_stats[years_active[-1]]["total"]
            if last > first:
                trend_analysis = "ØµØ¹ÙˆØ¯ÛŒ ğŸ“ˆ"
            elif last < first:
                trend_analysis = "Ù†Ø²ÙˆÙ„ÛŒ ğŸ“‰"
            else:
                trend_analysis = "Ø«Ø§Ø¨Øª â¡ï¸"
        
        return {
            "company_name": best_match,
            "match_score": best_score,
            "total_files": total_files,
            "years_active": years_active,
            "status_counts": status_counts.to_dict(),
            "sheet_distribution": sheet_distribution.to_dict(),
            "yearly_stats": dict(yearly_stats),
            "trend_analysis": trend_analysis,
            "recent_files": results.sort_values(["Ø³Ø§Ù„_Ø¹Ø¯Ø¯ÛŒ", "Ù…Ø§Ù‡"], ascending=[False, False]).head(15),
            "all_files": results
        }
    
    def get_all_years_report(self) -> Dict[str, Any]:
        """Ú¯Ø²Ø§Ø±Ø´ Ú©Ù„ÛŒ Ù‡Ù…Ù‡ Ø³Ø§Ù„â€ŒÙ‡Ø§"""
        if self.processed_data is None:
            return {}
        
        yearly_stats = defaultdict(lambda: {"total": 0, "Ø±Ø³Ù…ÛŒ": 0, "ØºÛŒØ±Ø±Ø³Ù…ÛŒ": 0, "Ù†Ø§Ù…Ø´Ø®Øµ": 0, "phones": 0})
        
        for _, row in self.processed_data.iterrows():
            year = row["Ø³Ø§Ù„_Ø¹Ø¯Ø¯ÛŒ"]
            if pd.isna(year):
                continue
            year = int(year)
            status = row["ÙˆØ¶Ø¹ÛŒØª"]
            yearly_stats[year]["total"] += 1
            yearly_stats[year][status] += 1
            yearly_stats[year]["phones"] += row["ØªØ¹Ø¯Ø§Ø¯_ØªÙ„ÙÙ†"]
        
        return dict(yearly_stats)
    
    def get_sheet_analysis(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´ÛŒØª Ù…Ù†Ø¨Ø¹"""
        if self.processed_data is None:
            return {}
        
        sheet_stats = {}
        for sheet_name in self.processed_data["Ø´ÛŒØª_Ù…Ù†Ø¨Ø¹"].unique():
            sheet_data = self.processed_data[self.processed_data["Ø´ÛŒØª_Ù…Ù†Ø¨Ø¹"] == sheet_name]
            
            sheet_stats[sheet_name] = {
                "total_files": len(sheet_data),
                "companies": len(sheet_data["Ù†Ø§Ù…_Ø´Ø±Ú©Øª"].unique()),
                "files_with_phones": len(sheet_data[sheet_data["ØªØ¹Ø¯Ø§Ø¯_ØªÙ„ÙÙ†"] > 0]),
                "phone_success_rate": len(sheet_data[sheet_data["ØªØ¹Ø¯Ø§Ø¯_ØªÙ„ÙÙ†"] > 0]) / len(sheet_data) * 100,
                "status_distribution": sheet_data["ÙˆØ¶Ø¹ÛŒØª"].value_counts().to_dict()
            }
        
        return sheet_stats
    
    def search_data(self, query: str, search_fields: List[str] = None) -> pd.DataFrame:
        """Search data based on query and specified fields"""
        if self.processed_data is None:
            return pd.DataFrame()
        
        if not query:
            return self.processed_data
        
        if search_fields is None:
            search_fields = ['Ù†Ø§Ù…_Ø´Ø±Ú©Øª', 'Ø´Ù…Ø§Ø±Ù‡_ØªÙ„ÙÙ†', 'Ù…Ø­ØµÙˆÙ„Ø§Øª', 'ÙØ§ÛŒÙ„_Ø§ØµÙ„ÛŒ', 'Ù†Ø§Ù…_Ù¾Ø§Ú©']
        
        # Create boolean mask for search
        mask = pd.Series([False] * len(self.processed_data))
        
        for field in search_fields:
            if field in self.processed_data.columns:
                field_mask = self.processed_data[field].astype(str).str.contains(
                    query, case=False, na=False, regex=False
                )
                mask = mask | field_mask
        
        return self.processed_data[mask]
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get basic statistics about the data"""
        if self.processed_data is None:
            return {}
        
        all_phones = []
        all_products = []
        
        for _, row in self.processed_data.iterrows():
            if row['Ø´Ù…Ø§Ø±Ù‡_ØªÙ„ÙÙ†'] != 'ÛŒØ§ÙØª Ù†Ø´Ø¯':
                phones = [p.strip() for p in str(row['Ø´Ù…Ø§Ø±Ù‡_ØªÙ„ÙÙ†']).split(',')]
                all_phones.extend(phones)
            
            if row['Ù…Ø­ØµÙˆÙ„Ø§Øª'] != 'ÛŒØ§ÙØª Ù†Ø´Ø¯':
                products = [p.strip() for p in str(row['Ù…Ø­ØµÙˆÙ„Ø§Øª']).split(',')]
                all_products.extend(products)
        
        stats = {
            'ØªØ¹Ø¯Ø§Ø¯_Ú©Ù„_Ø³Ù†Ø¯Ù‡Ø§': len(self.processed_data),
            'ØªØ¹Ø¯Ø§Ø¯_Ø´Ø±Ú©Øª_Ù‡Ø§ÛŒ_Ù…Ù†Ø­ØµØ±_Ø¨Ù‡_ÙØ±Ø¯': len(self.processed_data['Ù†Ø§Ù…_Ø´Ø±Ú©Øª'].unique()),
            'ØªØ¹Ø¯Ø§Ø¯_Ø³Ù†Ø¯Ù‡Ø§ÛŒ_Ø¨Ø§_ØªÙ„ÙÙ†': len(self.processed_data[self.processed_data['ØªØ¹Ø¯Ø§Ø¯_ØªÙ„ÙÙ†'] > 0]),
            'ØªØ¹Ø¯Ø§Ø¯_Ú©Ù„_ØªÙ„ÙÙ†_Ù‡Ø§': len(set(all_phones)),
            'ØªØ¹Ø¯Ø§Ø¯_Ú©Ù„_Ù…Ø­ØµÙˆÙ„Ø§Øª': len(set(all_products)),
            'Ø³Ø§Ù„_Ù‡Ø§ÛŒ_Ù…ÙˆØ¬ÙˆØ¯': sorted([int(x) for x in self.processed_data['Ø³Ø§Ù„_Ø¹Ø¯Ø¯ÛŒ'].dropna().unique() if not pd.isna(x)]),
            'Ù…Ø§Ù‡_Ù‡Ø§ÛŒ_Ù…ÙˆØ¬ÙˆØ¯': sorted(self.processed_data['Ù…Ø§Ù‡'].unique().tolist()),
            'Ø´ÛŒØª_Ù‡Ø§ÛŒ_Ù…Ù†Ø¨Ø¹': sorted(self.processed_data['Ø´ÛŒØª_Ù…Ù†Ø¨Ø¹'].unique().tolist()),
            'all_phones': list(set(all_phones)),
            'all_products': list(set(all_products))
        }
        
        return stats
